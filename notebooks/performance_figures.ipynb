{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse, Rectangle\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model performance\n",
    "There were 3 baselines:\n",
    "1. GPT3\n",
    "    * Without relation embedding\n",
    "    * With relation embedding\n",
    "2. OpenIE\n",
    "    * Without relation embedding\n",
    "    * With relation embedding\n",
    "3. My heuristic model\n",
    "    * Model relies on relation embedding, so only includes relation embedding\n",
    "\n",
    "Each was evaluated according to the following:\n",
    "1. GPT3\n",
    "    * Without relation embedding: evaluated without checking relation labels\n",
    "    * With relation embedding: evaluated both with and without checking relation labels\n",
    "2. OpenIE\n",
    "    * Without relation embedding: evaluated without checking relation labels\n",
    "    * With relation embedding: evaluated both with and without checking relation labels\n",
    "3. My heuristic model\n",
    "    * With relation embedding: evaluated with and without checking relation labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {\n",
    "    \n",
    "    'gpt3_no_embed':'../data/baselines/gpt3/10Mar_FINAL_TEST_no_rel_embed_performance.csv',\n",
    "    'gpt3_embed_check':'../data/baselines/gpt3/10Mar_FINAL_TEST_embedded_rel_yes_check_rels_performance.csv',\n",
    "    'gpt3_embed_no_check':'../data/baselines/gpt3/10Mar_FINAL_TEST_embedded_rel_no_check_rels_performance.csv',\n",
    "    #'openIE_no_embed':,\n",
    "    #'openIE_embed_check':,\n",
    "    #'openIE_embed_no_check',\n",
    "    'heuristic_check':'../data/distant_sup_output/09Mar_TEST_check_rels_performance.csv',\n",
    "    'heuristic_no_check':'../data/distant_sup_output/09Mar_TEST_no_check_rels_performance.csv',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_CI</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_CI</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1_CI</th>\n",
       "      <th>model_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011923</td>\n",
       "      <td>(0.0, 0.023809523809523808)</td>\n",
       "      <td>0.010914</td>\n",
       "      <td>(0.0, 0.02631578947368421)</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>(0.0, 0.024691358024691357)</td>\n",
       "      <td>gpt3_no_embed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>gpt3_embed_check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012005</td>\n",
       "      <td>(0.0, 0.023255813953488372)</td>\n",
       "      <td>0.011105</td>\n",
       "      <td>(0.0, 0.02631578947368421)</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>(0.0, 0.023676470588235285)</td>\n",
       "      <td>gpt3_embed_no_check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042277</td>\n",
       "      <td>(0.0, 0.1)</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>(0.0, 0.02564102564102564)</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>(0.0, 0.03541666666666665)</td>\n",
       "      <td>heuristic_check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041411</td>\n",
       "      <td>(0.0, 0.1)</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>(0.0, 0.02300475687103593)</td>\n",
       "      <td>0.016738</td>\n",
       "      <td>(0.0, 0.036111111111111066)</td>\n",
       "      <td>heuristic_no_check</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision                 precision_CI    recall  \\\n",
       "0   0.011923  (0.0, 0.023809523809523808)  0.010914   \n",
       "0   0.000000                   (0.0, 0.0)  0.000000   \n",
       "0   0.012005  (0.0, 0.023255813953488372)  0.011105   \n",
       "0   0.042277                   (0.0, 0.1)  0.011388   \n",
       "0   0.041411                   (0.0, 0.1)  0.010674   \n",
       "\n",
       "                    recall_CI        F1                        F1_CI  \\\n",
       "0  (0.0, 0.02631578947368421)  0.011292  (0.0, 0.024691358024691357)   \n",
       "0                  (0.0, 0.0)  0.000000                   (0.0, 0.0)   \n",
       "0  (0.0, 0.02631578947368421)  0.011423  (0.0, 0.023676470588235285)   \n",
       "0  (0.0, 0.02564102564102564)  0.017627   (0.0, 0.03541666666666665)   \n",
       "0  (0.0, 0.02300475687103593)  0.016738  (0.0, 0.036111111111111066)   \n",
       "\n",
       "            model_eval  \n",
       "0        gpt3_no_embed  \n",
       "0     gpt3_embed_check  \n",
       "0  gpt3_embed_no_check  \n",
       "0      heuristic_check  \n",
       "0   heuristic_no_check  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all dfs\n",
    "perf_dfs = {k:pd.read_csv(v) for k,v in data_paths.items()}\n",
    "# Add model/eval name as last column to all\n",
    "perf_df_list = []\n",
    "for model_eval, df in perf_dfs.items():\n",
    "    df['model_eval'] = model_eval\n",
    "    perf_df_list.append(df)\n",
    "# Concat\n",
    "overall_perfs = pd.concat(perf_df_list)\n",
    "overall_perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "I find these results very suspicious, for two reasons:\n",
    "1. The performance of the heuristic alg is higher when checking the relation labels than when not. This shouldn't be possible, as anything evaluated as correct when checking rel labels should still be correct when evaluated without checking.\n",
    "2. The performance of the heuristic algorithm is *drastically* worse than what it was the first time I ran it before making any of the changes that helped me drop way fewer sentences. Again, the performance should at least be the same, even if having access to more sentences didn't help, unless new sentences that weren't dropped before are now dropped, which I find that hard to beleive, as a huge number of sentences aren't dropped now. However, that original performance was evaluated on all documents, not just the test set; this observation could potentially be due to the specific documents in the test set? --> Should eval just the current test set out of the old predictions to test that hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, subplot_kw={'aspect': 'equal'}, figsize=(20, 20), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "######################### Make 2D plot for models with both values ##############################\n",
    "for label in full_laydown_order:\n",
    "    \n",
    "    # Get ellipse\n",
    "    ell_tup = f1_full_entries[label]\n",
    "    # Get color & linestyle\n",
    "    color_label = ' | '.join(label.split(' | ')[:-1])\n",
    "    color = full_entry_colors[color_label]\n",
    "    if label.split(' | ')[-1] == 'Original':\n",
    "        linestyle = ':'\n",
    "    else:\n",
    "        linestyle = '-'\n",
    "        \n",
    "    # If the relation performance and CI are zero, need to make a thin rectangle instead of an ellipse\n",
    "    if 0 in ell_tup:\n",
    "        rect_x = ell_tup[0] - 0.5*ell_tup[2]\n",
    "        shape = Rectangle((rect_x, 0), ell_tup[2], 0.02, edgecolor='black', linewidth=2,\n",
    "                          linestyle=linestyle, facecolor=color, label=color_label, alpha=0.5)\n",
    "    else:\n",
    "        shape = Ellipse((ell_tup[0], ell_tup[1]), ell_tup[2], ell_tup[3], edgecolor='black', linewidth=2,\n",
    "                        linestyle=linestyle, facecolor=color, label=color_label, alpha=0.5)\n",
    "    \n",
    "    # Add shape to axis\n",
    "    axs[0].add_artist(shape)\n",
    "\n",
    "axs[0].set_ylabel('Relation Performance')\n",
    "\n",
    "# Keep only the solid line version of each model/training set combination\n",
    "ellipses, labels = axs[0].get_legend_handles_labels()\n",
    "keep_labs, keep_ells = [], []\n",
    "for lab, ell in zip(full_laydown_order, ellipses):\n",
    "    if lab not in keep_labs and 'PICKLE' in lab:\n",
    "        keep_labs.append(lab)\n",
    "        keep_ells.append(ell)\n",
    "legend1 = axs[0].legend(handles=keep_ells, loc='upper left')\n",
    "# Dummy lines with NO entries, just to create the black style legend\n",
    "dummy_lines = []\n",
    "for linestyle, label in zip([':', '-'], ['Original', 'PICKLE']):\n",
    "    dummy_lines.append(Ellipse((0, 0), 0, 0, edgecolor=\"black\", linewidth=2, linestyle=linestyle, label=label,\n",
    "                              facecolor='white'))\n",
    "legend2 = axs[0].legend(dummy_lines, ['Original', 'PICKLE'])\n",
    "for legobj in legend2.legendHandles:\n",
    "    if legobj._label == 'PICKLE':\n",
    "        legobj.set_linewidth(1)\n",
    "axs[0].add_artist(legend1)\n",
    "axs[0].add_artist(legend2)\n",
    "\n",
    "############################################### Annotate plot #######################################\n",
    "orig_text = 'Performance on original\\ndomain for all models\\nis nearly identical'\n",
    "pick_text = 'On PICKLE, SciERC models\\noutperform GENIA models\\nfor entity extraction'\n",
    "ace_text = 'ACE05 models extract 0\\nrelations, and also have\\npoor entity extraction'\n",
    "\n",
    "axs[0].annotate('', xy=(0.8, 0), xytext=(0.8,0.45), arrowprops=dict(arrowstyle='<->', lw=2))\n",
    "axs[0].text(\n",
    "    0.64, 0.4, orig_text, ha=\"center\", va=\"center\", size=12,\n",
    "    bbox=dict(boxstyle='round', fc=\"white\", ec=\"black\", lw=2))\n",
    "\n",
    "axs[0].annotate('', xy=(0.3, 0), xycoords='data',\n",
    "            xytext=(0.55, 0.1), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"<->\",\n",
    "                            connectionstyle=\"bar,angle=360,fraction=-0.2\", lw=2))\n",
    "axs[0].text(\n",
    "    0.43, 0.22, pick_text, ha=\"center\", va=\"center\", size=12,\n",
    "    bbox=dict(boxstyle='round', fc=\"white\", ec=\"black\", lw=2))\n",
    "\n",
    "axs[0].annotate('', xy=(0.1, 0.03), xytext=(0.1,0.45), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "axs[0].text(\n",
    "    0.175, 0.5, ace_text, ha=\"center\", va=\"center\", size=12,\n",
    "    bbox=dict(boxstyle='round', fc=\"white\", ec=\"black\", lw=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obiewan",
   "language": "python",
   "name": "obiewan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
